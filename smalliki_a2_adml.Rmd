---
title: "assignment_2_adml"
author: "Santhosh reddy Mallikireddy"
date: "06/03/2020"
output: html_document
---
#collecting individual reviews into a list of string and also collecting review labels into a 'labels' list
```{r}
library(keras)

imdb_dir <- "C:/Users/santhosh/Desktop/aclImdb"
train_dir <- file.path(imdb_dir, "train")
labels <- c()
texts <- c()
for (label_type in c("neg", "pos")) {
  label <- switch(label_type, neg = 0, pos = 1)
  dir_name <- file.path(train_dir, label_type)
  for (fname in list.files(dir_name, pattern = glob2rx("*.txt"),
                           full.names = TRUE)) {
    texts <- c(texts, readChar(fname, file.info(fname)$size))
    labels <- c(labels, label)
  }
}
```

```{r}
### Tokenize the data

maxlen <- 150                 # We will cut reviews after 150 words
training_samples <- 100       # We will be training on 100 samples
validation_samples <- 10000   # We will be validating on 10000 samples
max_words <- 10000            # We will only consider the top 10,000 words in the dataset

tokenizer <- text_tokenizer(num_words = max_words) %>% 
  fit_text_tokenizer(texts)

sequences <- texts_to_sequences(tokenizer, texts)
word_index = tokenizer$word_index

cat("Found", length(word_index), "unique tokens.\n")
data <- pad_sequences(sequences, maxlen = maxlen)
y_data <- as.array(labels)
labels <- as.array(labels)
cat("Shape of data tensor:", dim(data), "\n")
cat('Shape of label tensor:', dim(labels), "\n")

```

# Split the data into a training set and a validation set
# But first, shuffle the data, since we started from data
# where sample are ordered (all negative first, then all positive).
```{r}

set.seed(123)
indices <- sample(1:nrow(data))
training_indices <- indices[1:training_samples]
validation_indices <- indices[(training_samples + 1): 
                                (training_samples + validation_samples)]

x_train <- data[training_indices,]
y_train <- labels[training_indices]

x_val <- data[validation_indices,]
y_val <- labels[validation_indices]

```

# Pre-process the embeddings
```{r}
glove_dir = '~/'
lines <- readLines(file.path(glove_dir, "glove.6B.100d.txt"))

embeddings_index <- new.env(hash = TRUE, parent = emptyenv())
for (i in 1:length(lines)) {
  line <- lines[[i]]
  values <- strsplit(line, " ")[[1]]
  word <- values[[1]]
  embeddings_index[[word]] <- as.double(values[-1])
}

cat("Found", length(embeddings_index), "word vectors.\n")

#building an embedding matrix that can be loaded into an embedding layer

embedding_dim <- 100
embedding_matrix <- array(0, c(max_words, embedding_dim))

for (word in names(word_index)) {
  index <- word_index[[word]]
  if (index < max_words) {
    embedding_vector <- embeddings_index[[word]]
    if (!is.null(embedding_vector))
      # Words not found in the embedding index will be all zeros.
      embedding_matrix[index+1,] <- embedding_vector
  }
}
```

#defining a model
```{r}
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

summary(model)

# Load the pretrained GloVe embeddings in the model
get_layer(model, index = 1) %>% 
  set_weights(list(embedding_matrix)) %>% 
  freeze_weights()

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

#evaluating on train model
history <- model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)
plot(history)
#by observation, validation accuracy is 56.51 by cutting reviews for first 150 words in every 100 samples. 
```

```{r}
#evaluating on test data
model %>% fit(
  x_train,
  y_train,
  epochs = 2,
  batch_size = 32)
result <- model %>%  evaluate(data,y_data)
result # Test Acuuracy of the model is 51%
```


#using embedding layer
```{r}
# Using an embedding layer and classifier on the IMDB data
model <- keras_model_sequential() %>% 
  layer_embedding(input_dim = max_words, output_dim = embedding_dim, 
                  input_length = maxlen) %>% 
  layer_flatten() %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

```

```{r}
history1 <- model %>% fit(
  x_train, y_train,
  epochs = 20,
  batch_size = 32,
  validation_data = list(x_val, y_val)
)

plot(history1)
#by observation, validation accuracy is 55.81 by cutting reviews for first 150 words in every 100 samples. 
```

```{r}
#testdata evaluation
model %>% fit(
  x_train,
  y_train,
  epochs = 2,
  batch_size = 32)
result1 <- model %>%  evaluate(data,y_data)
result1 # Test Accuracy of the model is 51%

#With fewer set of training samples model quickly starts overfitting. model accuracy using both pretrained embedding layer and embedding layer is same when we take fewer samples.Model performance depends on the number of samples we take and when we take few samples it depends on which 100 samples we choose.
```

