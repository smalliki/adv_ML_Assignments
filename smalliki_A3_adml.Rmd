---
title: "assignment_3"
author: "Santhosh reddy Mallikireddy"
date: "03/05/2020"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

#Loading the data
```{r}
library(tibble)
library(readr)
library(keras)
data <- read_csv("jena_climate_2009_2016.csv")
glimpse(data)


```

Here is the plot of temperature (in degrees Celsius) over time. On this plot, you can clearly see the yearly periodicity of temperature
```{r}
library(ggplot2)
ggplot(data, aes(x = 1:nrow(data), y = `T (degC)`)) + geom_line()
```
On this plot, you can clearly see the yearly periodicity of temperature from 2009 to 2016.


Here is a more narrow plot of the first ten days of temperature data (since the data is recorded every ten minutes, we get 144 data points per day):
```{r}
ggplot(data[1:1440,], aes(x = 1:1440, y = `T (degC)`)) + geom_line()
```
On this plot, you can see daily periodicity, especially evident for the last 4 days. We can also note that this ten-days period must be coming from a fairly cold winter month.

If we were trying to predict average temperature for the next month given a few month of past data, the problem would be easy, due to the reliable year-scale periodicity of the data. But looking at the data over a scale of days, the temperature looks a lot more chaotic. 

So our aim is to see if this data is predictable at a daily scale.

Since the dataset we have is huge we are taking only 20000 samples.
```{r}
data <- data.matrix(data[,-1])
```

#Data preprocessing
we preprocess the data by subtracting the mean of each timeseries and dividing by the standard deviation. we are going to use the first 20,000 timesteps as training data, so compute the mean and standard deviation for normalization only on this fraction of the data.
```{r}
train_data <- data[1:20000,]
mean <- apply(train_data, 2, mean)
std <- apply(train_data, 2, sd)
data <- scale(data, center = mean, scale = std)
```


```{r}
generator <- function(data, lookback, delay, min_index, max_index,
                      shuffle = FALSE, batch_size = 128, step = 6) {
  if (is.null(max_index))
    max_index <- nrow(data) - delay - 1
  i <- min_index + lookback
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+lookback):max_index), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + lookback
      rows <- c(i:min(i+batch_size-1, max_index))
      i <<- i + length(rows)
    }
    
    samples <- array(0, dim = c(length(rows), 
                                lookback / step,
                                dim(data)[[-1]]))
    targets <- array(0, dim = c(length(rows)))
                     
    for (j in 1:length(rows)) {
      indices <- seq(rows[[j]] - lookback, rows[[j]] - 1, 
                     length.out = dim(samples)[[2]])
      samples[j,,] <- data[indices,]
      targets[[j]] <- data[rows[[j]] + delay,2]
    }            
    
    list(samples, targets)
  }
}
```
The i variable contains the state that tracks next window of data to return, so it is updated using superassignment (e.g. i <<- i + length(rows)).

Now, let’s use the abstract generator function to instantiate three generators: one for training, one for validation, and one for testing. Each will look at different temporal segments of the original data: the training generator looks at the first 15000 timesteps, the validation generator looks at the following 2000, and the test generator looks at the remainder.

We will use the following parameter values:

lookback = 1440, i.e. our observations will go back 10 days.
steps = 6, i.e. our observations will be sampled at one data point per hour.
delay = 144, i.e. our targets will be 24 hours in the future
```{r}
lookback <- 1440
step <- 6 #The period, in timesteps, at which you sample data. You’ll set it 6 in order to draw one data point every hour.
delay <- 144
batch_size <- 128
train_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 1,
  max_index = 15000,
  shuffle = TRUE,
  step = step, 
  batch_size = batch_size
)
val_gen = generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 15001,
  max_index = 17000,
  step = step,
  batch_size = batch_size
)
test_gen <- generator(
  data,
  lookback = lookback,
  delay = delay,
  min_index = 17001,
  max_index = 20000,
  step = step,
  batch_size = batch_size
)
# This is how many steps to draw from `val_gen`
# in order to see the whole validation set:
val_steps <- (17000 - 15001 - lookback) / batch_size
  # This is how many steps to draw from `test_gen`
# in order to see the whole test set:
test_steps <- (nrow(data) - 17001 - lookback) / batch_size
```

#Non Machine Learning Baseline

In our case, the temperature timeseries can safely be assumed to be continuous (the temperatures tomorrow are likely to be close to the temperatures today) as well as periodical with a daily period. Thus a common sense approach would be to always predict that the temperature 24 hours from now will be equal to the temperature right now. Let’s evaluate this approach, using the Mean Absolute Error metric (MAE). Mean Absolute Error is simply equal to:
```{r}
evaluate_naive_method <- function() {
  batch_maes <- c()
  for (step in 1:val_steps) {
    c(samples, targets) %<-% val_gen()
    preds <- samples[,dim(samples)[[2]],2]
    mae <- mean(abs(preds - targets))
    batch_maes <- c(batch_maes, mae)
  }
  print(mean(batch_maes))
}

evaluate_naive_method()
```
MAE values is 0.38 for this method. Which means base performance of our model is 0.38.
Since our temperature data has been normalized to be centered on 0 and have a standard deviation of one, this number is not immediately interpretable. That’s a fairly large average absolute error – now the game is to leverage our knowledge of deep learning to do better.



#A basic machine learning approach

The following listing shows a fully connected model that starts by flattening the data and then runs it through two dense layers. Note the lack of activation function on the last dense layer, which is typical for a regression problem. You use MAE as the loss. Because you’re evaluating on the exact same data and with the exact same metric you did with the common-sense approach, the results will be directly comparable.
```{r}
model <- keras_model_sequential() %>% 
  layer_flatten(input_shape = c(lookback / step, dim(data)[-1])) %>% 
  layer_dense(units = 32, activation = "relu") %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 500,
  epochs = 10,
  validation_data = val_gen,
  validation_steps = val_steps
)
plot(history)
```

Some of our validation losses are more than the no-learning baseline,this is not reliable. This goes to show the merit of having had this baseline in the first place: it turns out not to be so easy to outperform.

#Using stacked recurrent dropouts

Increasing network capacity is typically done by increasing the number of units in the layers, or adding more layers. Recurrent layer stacking is a classic way to build more powerful recurrent networks.

In this model we are using layer_gru with dropout = 0.1 and units = 64,128 in respective layers.
```{r}
model <- keras_model_sequential() %>% 
  layer_gru(units = 64, 
            dropout = 0.1, 
            recurrent_dropout = 0.5,
            return_sequences = TRUE,
            input_shape = list(NULL, dim(data)[[-1]])) %>% 
  layer_gru(units = 128, activation = "relu",
            dropout = 0.1,
            recurrent_dropout = 0.5) %>% 
  layer_dense(units = 1)

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "mae"
)

history <- model %>% fit_generator(
  train_gen,
  steps_per_epoch = 50,
  epochs = 5,
  validation_data = val_gen,
  validation_steps = val_steps
)
```
We can see that the added layers does improve ours results.Since we are not overfitting, we could safely increase the size of our layers, in quest for a bit of validation loss improvement. This does have a non-negligible computational cost, though.

```{r}
plot(history)
```

Evaluating on the Test set
```{r}
history <- model %>% fit_generator(
  test_gen,
  steps_per_epoch = 50,
  epochs = 3,
  validation_steps = test_steps
)

```

```{r}
plot(history)
```

